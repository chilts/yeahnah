{
  "name": "ger",
  "description": "Good Enough Recommendations (GER) is a collaborative filtering based recommendations engine built to be easy to use and integrate into your application.",
  "url": "https://github.com/grahamjenson/ger.git",
  "keywords": [
    "recommend",
    "recommended",
    "recommendation",
    "engine",
    "collaborative",
    "filtering"
  ],
  "author": {
    "name": "Graham Jenson",
    "email": "grahamjenson@maori.geek.nz"
  },
  "dependencies": {
    "bluebird": "2.3.4",
    "knex": "0.7.0",
    "pg": "3.5.0",
    "rethinkdbdash": "1.15.3",
    "pg-copy-streams": "0.3.0",
    "underscore": "1.7.0",
    "moment": "2.8.3",
    "split": "0.3.0"
  },
  "devDependencies": {
    "q-io": "1.11.3",
    "mocha": "1.18.2",
    "chai": "1.9.1",
    "coffee-script": "1.7.1"
  },
  "scripts": {
    "test": "mocha"
  },
  "main": "ger.coffee",
  "version": "0.0.62",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/grahamjenson/ger.git"
  },
  "readme": "\n<img src=\"./assets/ger300x200.png\" align=\"right\" alt=\"GER logo\" />\n\nProviding good recommendations can get greater user engagement and provide an opportunity to add value that would otherwise not exist. The main reason why many applications don't provide recommendations is the perceived difficulty in either implementing a custom engine or using an off-the-shelf engine.\n\nGood Enough Recommendations (**GER**) is an attempt to reduce this difficulty by providing a recommendation engine that is scalable, easily usable and easy to integrate. GER's primary goal is to generate **good enough** recommendations for your application or product.\n\nPosts about (or related to) GER:\n\n1. Demo Movie Recommendations Site: [Yeah, Nah](http://yeahnah.maori.geek.nz/)\n1. Overall description and motivation of GER: [Good Enough Recommendations with GER](http://maori.geek.nz/post/good_enough_recomendations_with_ger)\n2. How GER works [GER's Anatomy: How to Generate Good Enough Recommendations](http://www.maori.geek.nz/post/how_ger_generates_recommendations_the_anatomy_of_a_recommendations_engine)\n2. Testing frameworks being used to test GER: [Testing Javascript with Mocha, Chai, and Sinon](http://www.maori.geek.nz/post/introduction_to_testing_node_js_with_mocha_chai_and_sinon)\n3. Bootstrap function for dumping data into GER: [Streaming directly into Postgres with Hapi.js and pg-copy-stream](http://www.maori.geek.nz/post/streaming_directly_into_postgres_with_hapi_js_and_pg_copy_stream)\n4. [Postgres Upsert (Update or Insert) in GER using Knex.js](http://www.maori.geek.nz/post/postgres_upsert_update_or_insert_in_ger_using_knex_js)\n\n#Quick Start Guide\n\n**All functions from GER return a promise.**\n\nInstall `ger` with `npm`:\n\n```bash\nnpm install ger\n```\n\nrequire `ger`:\n\n```javascript\nvar g = require('ger')\nvar GER = g.GER\n```\n\nInitialize an in memory Event Store Manager (ESM):\n\n```javascript\nvar esm = new g.MemESM()\nesm.initialize()\n```\n\nThen create the GER instance by passing the ESM:\n\n```javascript\nvar ger = new GER(esm);\n```\n\nUsing the promises library [bluebird](https://www.npmjs.org/package/bluebird) (`bb`) we can add some events and get some recommendations:\n\n```javascript\nbb.all([\n ger.action('view', 1),\n ger.action('buy', 10),\n\n ger.event('p1','view','a'), \n ger.event('p2','view','a'),\n ger.event('p2','buy','a')\n])\n.then( function() {\n // What should 'p1' 'buy'?\n return ger.recommendations_for_person('p1', 'buy') \n})\n.then( function(recommendations) {\n  // {recommendations: [{thing: 'a', weight: '.75'}]}\n})\n```\n\n\n#The GER Model\n\n*I am sorry for the formality, but formal models are the easiest way to remove ambiguity and describe precisely what is going on.*\n\nThe core sets of GER are:\n\n1. `P` people\n2. `T` things\n3. `A` actions\n\nEvents are in the set that is the Cartesian product of these, i.e. `P × A × T`. For example, when `bob` `like`s the `hobbit` movie, this is represented with the event `<bob, like, hobbit>`.\n\nThe history of any given person is all the `thing`'s they have `action`ed in the form `<action,thing>`, i.e. `A × T`. The function `H` takes a person and returns their history. For example the history for `bob` after he `like`d the `hobbit` would be `H(bob) = {<like, hobbit>}`. \n\nThe [Jaccard similarityy](http://en.wikipedia.org/wiki/Jaccard_index) metric, defined as the function `J`, is used to calculate the similarity between people using their histories. That is, the similarity between two people `p1`, `p2` is the Jaccard metric between their two histories `J(p1,p2) = (|H(p1) INTERSECTION H(p2)| / |H(p1) UNION H(p2)|)`. \n\nFor example, given that `bob` `like`d the `hobbit` and `hate`d the `x-men`, where `alice` only `hate`d the `x-men`. \n\n1. `H(alice) = {<hate, x-men>}`\n2. `H(bob) = {<like, hobbit>, <hate, x-men>}`\n3. `H(bob) INTERSECTION H(alice) = {<hate, x-men>}` with cardinality `1`\n4. `H(bob) UNION H(alice) = {<like, hobbit>, <hate, x-men>}` with cardinality `2`\n5. The similarity between `bob` and `alice` is therefore `J(bob,alice) = 1/2`\n\nJaccard similarity is a proper [metric](http://en.wikipedia.org/wiki/Metric_(mathematics)), so it is comes with many useful properties like *symmetry* where `J(bob, alice) = J(alice,bob)`.\n\nIt is also useful to define *similarity*:\n\n1. Two people are said to be **similar** if the have a non-zero Jaccard similarity\n\n### Recommendations\n\nRecommendations are a set of weighted `thing`s which are calculated for a person `p` and action `a` using the function `R(p,a)`. The weight of a thing `t` is the sum of the similarities between the person `p` and all people who have `<a, t>` in their history. One additional constraint on `R` is that it only returns non-zero weighted recommendations.\n\nFor Example, given that:\n\n1. `bob` `like`s the `x-men` but `hate`s `harry potter`.\n2. `alice` `hate`s `harry potter`, and `like`s the `x-men` and `avengers`\n3. `carl` `like`s the `x-men`, the `avengers` and `batman`\n \nWhat should be movie recommendations should `bob` `like`, i.e. `R(bob,like)`? We can calculate that:\n\n1. `J(bob,bob) = 1`\n1. `J(bob,alice) = 2/3`\n2. `J(bob,carl) = 1/4`\n\n`bob` has three potential recommendations to `like`: `x-men`, `avengers` and `batman`. For each of these we can calculate the weight that `bob` will `like` them:\n\n1. `x-men` is `J(bob,bob) + J(bob,alice) + J(bob,carl) = 1.92`\n2. `avengers` is `J(bob,alice) + J(bob,carl) = 0.92`\n3. `batman` is `J(bob,carl) = 0.25`\n\nTherefore, the recommendations for `bob` to `like` are `R(bob,like) = {<x-men, 1.92>, <avengers, 0.92>, <batman, 0.25>}`. Even though `bob` has seen `x-men` it has been included in the recommendations because he does `like` it. This would make sense if the recommendations were for something that could be consumed multiple times, like food or music.\n\n#Practical Changes to the Model\n\nThe above model is simple and wouldn't be able to deal with some of the real world requirements and limitations. Therefore, some additional features and required limitations to the model to make it practical have been applied. \n\n## Additional Features\n\nIn the simple model each action is treated equally when measuring a persons similarity to another; this is not the case in reality. If two people `like`d the same thing they may be more similar than if they `hate`d the same thing. By weighting each action, and finding the Jaccard similarity **per-action** then combining the results with respect to the action's weight, the similarity function can more accurately represent reality.   \n\n**When** an event occurs is a very important concept ignored in the simple model. If a person `like`d the `hobbit` today, and a `x-men` last year; they are probably more receptive to recommendations like the `hobbit`. To handle this, every event has an attached date of when it most recently occurred and:\n\n* The most recent events (defined using a variable for a number of days) are weighted higher than past events, done by calculating multiple Jaccard similarities with a weighted mean. *Note: this may break the symmetry of our similarity function, further mathematicians are required*\n \nRecommending something that a person has already actioned (e.g. bought) could be undesirable. By providing a list of the actions to filter recommendations, selected recommendations can be removed if they occurred in a persons history. For example, it makes sense to filter `hate` actions to stop recommending things they clearly don't want. However, they could potentially still receive recommendations for things they may have already `like`d, because every year they might like to re-watch movies again.\n\nA single highly similar person can cause bad recommendations by disproportionately dominating the recommendation weights. To mitigate this risk GER encourages recommendations that are recommended by multiple people. Using the `crowd_weight` concept which makes the weight of a recommendation more than just the sum of the peoples weights by also including how many people recommended it.\n\n## Limitations\n\nWhen dealing with large sets of data practical limitations are necessary to ensure performance. Here is the list of limitations imposed on the above model and features.\n\n### Model Limitations\n\nThe first limitation is to not generate recommendations for a person that has under a **minimum amount of history**. For example, if a person has only `like`d one movie, their generated recommendations will probably be random. In this case GER return no recommendations and lets the client handle this situation.\n\nThe most expensive aspect of GER is finding and calculating similarity between people. This is especially expensive for any person who has a large history **and every person they are similar to**. Given that a person with a large history is similar to many people, only a few such people can significantly decrease the performance of the entire engine. To ensure this is not the case, a few limitations were put in place:\n\n1. Limit the number of similar people to find, while attempting to find the most similar people for a users recent activity\n2. Limit the size of the history when calculating similarities\n\nFinding and weighting every potential recommendation from all similar people may also be expensive and returning every recommendation is likely superfluous. For this the limitations in place are:\n\n1. Only recommend the most recent events from the similar users\n2. Only return a number of the best recommendations\n\nLimiting the number of similar people, the length of their history, and the amount of recommendations to find, all have different performance and accuracy impacts **per data-set**. Finding the best values for these is a learning process through trial and error. \n\nAn important aspect to note about these limits is that they may create the potential for abuse and malicious manipulation of the recommendations. A way to see this is by considering a person who `hate`s all movies, but only `like`s one. The implications of such a user are:\n\n1. They will be similar to all people who have `hate`d anything\n2. Due to limiting history size, they may be a much higher similarity than they would otherwise have been\n3. Every person would include in their potential recommendations the movie the malicious person `like`s\n\nTherefore, a person who profits from manipulating recommendations of other users, may attempt to manipulate the system this way.\n\n###Data-set Compacting Limitations\n\nGiven the above description, it is cleat that some events will never be used. For example, if the event are old or belong to a user who has a long history they will not be used in any calculations. These events just loiter, take up space and slow calculations down. By trying to identify these events with some basic heuristics and removing them, it can dramatically speed up performance and decreases the size of the data-set. I call these compacting algorithms.\n\nCurrently there are two main compacting algorithms:\n\n1. Limit the number of events per person, per action, e.g. ensure `bob` has a maximum of 1000 `hate`s.\n1. Limit the number of events per thing, per action, e.g. ensure that a `hobbit` only has 1000 `hate`s.\n\nThese compacting algorithms delete the oldest events first as newer events carry more practical importance. They also solve the problem stated above about the malicious user who `hate`s everything, as their history will be reduced and they will be similar to less people.\n\nLike the other limitations, the numbers associated with the compacting limitations are data-set specific, and can probably be best found through trial and error.\n\n#The Algorithmic Description\n\nThe API for recommendations follows the core model and accepts a `person` and an `action` and returns a list of weighted things by following these steps:\n\n1. Find similar people to `person` by looking at their history (*limiting the number of returned similar people*)\n2. Calculate the similarities from `person` to the list of people (*limiting the amount of history*)\n3. Find a list of the most recent `thing`s the similar people have `action`ed (*limiting the number returned*)\n4. Calculating the weights of `thing`s using the similarity of the people (*additionally weighting the `crowd_weight`, filtering based on filter actions, then retuning the highest weighted*)\n\n# Technology\n\nGER is implemented in Coffee-Script on top of Node.js ([here](http://www.maori.geek.nz/post/why_should_you_use_coffeescript_instead_of_javascript) are my reasons for using Coffee-Script). The core logic is implemented in an abstractions called an Event Store Manager (**ESM**), this is the persistency and many calculations occur.\n\nCurrently there is an in memory ESM and a PostgreSQL ESM. There is also a RethinkDB ESM in the works being implemented by the awesome [linuxlich](https://github.com/thelinuxlich/ger).\n\n##Event Store Manager\n\nThe API for Initialization\n\n1. `esm = new ESM(namespace, options = {})`\n2. `initialize()` will create resources necessary for ESM to function for namespace\n3. `destroy()` will destroy all resources for ESM in namespace\n4, `exists()` will check if the namespace exists\n\nThe API for the ESM to generate recommendations is:\n\n1. `get_actions()` returns the actions with weights e.g. {'like': 1}\n2. `find_similar_people(person, action, actions, limits...)` returns \n3. `calculate_similarities_from_person(person, people, actions, limits...)`\n4. `recently_actioned_things_by_people(people)`\n5. `person_history_count`\n6. `filter_things_by_previous_actions`\n\nThe API for the ESM to insert data is:\n\n1. `add_event`\n2. `count_events` and `estimate_event_count`\n2. `set_action_weight` (and `get_action_weight`)\n3. `bootstrap`\n\nThe API for the ESM to search and manipulate the events store:\n\n1. `find_events(person, action, thing)` at least one of the arguments must be provided\n2. `delete_events(person, action, thing)` at least one of the arguments must be provided\n\nThe API for the ESM to compact the database is:\n\n1. `pre_compact`\n2. `compact_people`\n3. `compact_things`\n4. `expire_events`\n5. `post_compact`\n\n\n#Changelog\n\n2014-12-30 - added find and delete events methods.\n\n2014-12-22 - added exists to check if namespace is initilaized. also changed some indexes in rethinkdb, and changed some semantics around initialize \n\n2014-12-22 - Added Rethink DB Event Store Manager.\n\n2014-12-9 - Added more explanation to the returned recommendations so they can be reasoned about externally\n\n2014-12-4 - Changed ESM API to be more understandable and also updated README\n\n2014-11-27 - Started returning the last actioned at date with recommendations\n\n2014-11-25 - Added better way of selecting recommendations from similar people.\n\n2014-11-12 - Added better heuristic to select related people. Meaning less related people need to be selected to find good values\n",
  "readmeFilename": "README.md",
  "gitHead": "75b33e6a792437cd2d9f22829da2116238f760a4",
  "bugs": {
    "url": "https://github.com/grahamjenson/ger/issues"
  },
  "homepage": "https://github.com/grahamjenson/ger",
  "_id": "ger@0.0.62",
  "_shasum": "8151776f7cc85d9c954a3b22ada520cfc39ed560",
  "_from": "ger@0.0.62"
}
